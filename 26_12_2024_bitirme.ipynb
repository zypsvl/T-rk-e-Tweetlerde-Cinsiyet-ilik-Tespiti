{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnR5d6UXSsq-",
        "outputId": "971cc151-f2ac-4cf1-b920-68e1735ba884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kullanılan cihaz: cuda\n",
            "Temizlenmiş veri yükleniyor...\n",
            "Orijinal veri yükleniyor...\n",
            "Label sütunu one-hot encode ediliyor...\n",
            "Veriler hazırlanıyor...\n",
            "BERT modeli ve tokenizer yükleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Veriler tokenize ediliyor...\n",
            "BERT embedding'leri GPU üzerinde hesaplanıyor...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertModel, TFBertForSequenceClassification\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch CUDA Bellek Yönetimi\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# GPU Kullanımını Kontrol Et\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Kullanılan cihaz: {device}\")\n",
        "\n",
        "# Temizlenmiş Veri Yükleme\n",
        "print(\"Temizlenmiş veri yükleniyor...\")\n",
        "df_cleaned = pd.read_csv('/content/processed_text.csv', encoding='utf-8')\n",
        "\n",
        "# Orijinal Veri Yükleme ve Label Kolonunu Ekleme\n",
        "print(\"Orijinal veri yükleniyor...\")\n",
        "df_original = pd.read_csv('train_0.csv', encoding='utf-8')\n",
        "df_cleaned['Label'] = df_original['Label']\n",
        "\n",
        "# Label Kolonunu One-Hot Encode\n",
        "print(\"Label sütunu one-hot encode ediliyor...\")\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_labels = one_hot_encoder.fit_transform(df_cleaned['Label'].values.reshape(-1, 1))\n",
        "\n",
        "# Eğitim ve Test Verisi Ayrımı\n",
        "print(\"Veriler hazırlanıyor...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_cleaned['Processed_Text'], one_hot_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# BERT Tokenizer ve Model Yükleme\n",
        "print(\"BERT modeli ve tokenizer yükleniyor...\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
        "bert_model = BertModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\").to(device)\n",
        "\n",
        "# Tokenize ve Pad İşlemi\n",
        "print(\"Veriler tokenize ediliyor...\")\n",
        "def tokenize_and_pad(texts, tokenizer, max_len=128):\n",
        "    tokens = tokenizer(texts.tolist(), max_length=max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "X_train_ids, X_train_mask = tokenize_and_pad(X_train, bert_tokenizer)\n",
        "X_test_ids, X_test_mask = tokenize_and_pad(X_test, bert_tokenizer)\n",
        "\n",
        "# Verileri GPU'ya Taşı\n",
        "X_train_ids, X_train_mask = X_train_ids.to(device), X_train_mask.to(device)\n",
        "X_test_ids, X_test_mask = X_test_ids.to(device), X_test_mask.to(device)\n",
        "\n",
        "# BERT'ten Embedding Alma\n",
        "print(\"BERT embedding'leri GPU üzerinde hesaplanıyor...\")\n",
        "def process_in_batches(input_ids, attention_mask, model, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(input_ids), batch_size):\n",
        "        batch_input_ids = input_ids[i:i + batch_size]\n",
        "        batch_attention_mask = attention_mask[i:i + batch_size]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "            embeddings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "X_train_embeddings = process_in_batches(X_train_ids, X_train_mask, bert_model)\n",
        "X_test_embeddings = process_in_batches(X_test_ids, X_test_mask, bert_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kYc_0crV8uE",
        "outputId": "add61e27-2924-4ae3-e272-41934194079a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN modeli eğitiliyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5222 - loss: 0.9929 - val_accuracy: 0.5464 - val_loss: 0.9521\n",
            "Epoch 2/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5296 - loss: 0.9780 - val_accuracy: 0.5467 - val_loss: 0.9364\n",
            "Epoch 3/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5281 - loss: 0.9684 - val_accuracy: 0.5464 - val_loss: 0.9310\n",
            "Epoch 4/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5306 - loss: 0.9632 - val_accuracy: 0.5475 - val_loss: 0.9209\n",
            "Epoch 5/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5367 - loss: 0.9493 - val_accuracy: 0.5547 - val_loss: 0.9208\n",
            "Epoch 6/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5442 - loss: 0.9402 - val_accuracy: 0.5558 - val_loss: 0.9151\n",
            "Epoch 7/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5360 - loss: 0.9429 - val_accuracy: 0.5625 - val_loss: 0.9168\n",
            "Epoch 8/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5395 - loss: 0.9373 - val_accuracy: 0.5617 - val_loss: 0.9204\n",
            "Epoch 9/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5444 - loss: 0.9345 - val_accuracy: 0.5650 - val_loss: 0.9197\n",
            "Epoch 10/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5476 - loss: 0.9268 - val_accuracy: 0.5528 - val_loss: 0.9132\n",
            "LSTM modeli eğitiliyor...\n",
            "Epoch 1/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 48ms/step - accuracy: 0.5159 - loss: 1.0019 - val_accuracy: 0.5464 - val_loss: 0.9582\n",
            "Epoch 2/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - accuracy: 0.5290 - loss: 0.9860 - val_accuracy: 0.5464 - val_loss: 0.9536\n",
            "Epoch 3/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.5344 - loss: 0.9743 - val_accuracy: 0.5464 - val_loss: 0.9508\n",
            "Epoch 4/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.5343 - loss: 0.9694 - val_accuracy: 0.5464 - val_loss: 0.9540\n",
            "Epoch 5/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.5252 - loss: 0.9784 - val_accuracy: 0.5464 - val_loss: 0.9465\n",
            "Epoch 6/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.5346 - loss: 0.9679 - val_accuracy: 0.5469 - val_loss: 0.9450\n",
            "Epoch 7/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - accuracy: 0.5248 - loss: 0.9679 - val_accuracy: 0.5464 - val_loss: 0.9431\n",
            "Epoch 8/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - accuracy: 0.5325 - loss: 0.9657 - val_accuracy: 0.5472 - val_loss: 0.9381\n",
            "Epoch 9/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - accuracy: 0.5371 - loss: 0.9606 - val_accuracy: 0.5475 - val_loss: 0.9365\n",
            "Epoch 10/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.5383 - loss: 0.9550 - val_accuracy: 0.5489 - val_loss: 0.9316\n",
            "GRU modeli eğitiliyor...\n",
            "Epoch 1/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.5252 - loss: 0.9953 - val_accuracy: 0.5464 - val_loss: 0.9627\n",
            "Epoch 2/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 43ms/step - accuracy: 0.5300 - loss: 0.9859 - val_accuracy: 0.5464 - val_loss: 0.9528\n",
            "Epoch 3/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 45ms/step - accuracy: 0.5294 - loss: 0.9817 - val_accuracy: 0.5464 - val_loss: 0.9473\n",
            "Epoch 4/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 43ms/step - accuracy: 0.5350 - loss: 0.9695 - val_accuracy: 0.5464 - val_loss: 0.9442\n",
            "Epoch 5/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.5363 - loss: 0.9678 - val_accuracy: 0.5464 - val_loss: 0.9423\n",
            "Epoch 6/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.5308 - loss: 0.9745 - val_accuracy: 0.5458 - val_loss: 0.9428\n",
            "Epoch 7/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 43ms/step - accuracy: 0.5324 - loss: 0.9623 - val_accuracy: 0.5447 - val_loss: 0.9335\n",
            "Epoch 8/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 43ms/step - accuracy: 0.5463 - loss: 0.9516 - val_accuracy: 0.5500 - val_loss: 0.9434\n",
            "Epoch 9/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.5276 - loss: 0.9576 - val_accuracy: 0.5478 - val_loss: 0.9311\n",
            "Epoch 10/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 43ms/step - accuracy: 0.5418 - loss: 0.9541 - val_accuracy: 0.5464 - val_loss: 0.9355\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            "  Model  Precision    Recall  F1-Score  Accuracy\n",
            "0   CNN   0.475075  0.552778  0.408520  0.552778\n",
            "1  LSTM   0.452820  0.548889  0.400456  0.548889\n",
            "2   GRU   0.298541  0.546389  0.386114  0.546389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# CNN Modeli\n",
        "print(\"CNN modeli eğitiliyor...\")\n",
        "cnn_model = Sequential([\n",
        "    Embedding(input_dim=len(bert_tokenizer.vocab), output_dim=128, input_length=128),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(one_hot_labels.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_history = cnn_model.fit(X_train_embeddings, y_train, epochs=10, batch_size=16, validation_data=(X_test_embeddings, y_test))\n",
        "\n",
        "# LSTM Modeli\n",
        "print(\"LSTM modeli eğitiliyor...\")\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=len(bert_tokenizer.vocab), output_dim=128, input_length=128),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(one_hot_labels.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_history = lstm_model.fit(X_train_embeddings, y_train, epochs=10, batch_size=16, validation_data=(X_test_embeddings, y_test))\n",
        "\n",
        "# GRU Modeli\n",
        "print(\"GRU modeli eğitiliyor...\")\n",
        "gru_model = Sequential([\n",
        "    Embedding(input_dim=len(bert_tokenizer.vocab), output_dim=128, input_length=128),\n",
        "    Bidirectional(GRU(64, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(one_hot_labels.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "gru_history = gru_model.fit(X_train_embeddings, y_train, epochs=10, batch_size=16, validation_data=(X_test_embeddings, y_test))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjIGrRLciC0q",
        "outputId": "bbd57ed6-bfa8-4c61-9947-38f88422432a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-LSTM modeli eğitiliyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 44ms/step - accuracy: 0.5290 - loss: 0.9954 - val_accuracy: 0.5464 - val_loss: 0.9595\n",
            "Epoch 2/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - accuracy: 0.5323 - loss: 0.9805 - val_accuracy: 0.5464 - val_loss: 0.9589\n",
            "Epoch 3/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - accuracy: 0.5346 - loss: 0.9753 - val_accuracy: 0.5464 - val_loss: 0.9478\n",
            "Epoch 4/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.5310 - loss: 0.9780 - val_accuracy: 0.5464 - val_loss: 0.9479\n",
            "Epoch 5/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.5359 - loss: 0.9689 - val_accuracy: 0.5464 - val_loss: 0.9453\n",
            "Epoch 6/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - accuracy: 0.5295 - loss: 0.9751 - val_accuracy: 0.5464 - val_loss: 0.9448\n",
            "Epoch 7/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 43ms/step - accuracy: 0.5257 - loss: 0.9647 - val_accuracy: 0.5464 - val_loss: 0.9433\n",
            "Epoch 8/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - accuracy: 0.5326 - loss: 0.9610 - val_accuracy: 0.5464 - val_loss: 0.9591\n",
            "Epoch 9/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.5325 - loss: 0.9624 - val_accuracy: 0.5464 - val_loss: 0.9355\n",
            "Epoch 10/10\n",
            "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.5330 - loss: 0.9579 - val_accuracy: 0.5469 - val_loss: 0.9321\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m  4/113\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 22ms/step"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            "\u001b[1m  8/113\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 17ms/step"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
            "     Model  Precision    Recall  F1-Score  Accuracy\n",
            "0      CNN   0.475075  0.552778  0.408520  0.552778\n",
            "1     LSTM   0.452820  0.548889  0.400456  0.548889\n",
            "2      GRU   0.298541  0.546389  0.386114  0.546389\n",
            "3  Bi-LSTM   0.436075  0.546944  0.400872  0.546944\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Bi-LSTM Modeli\n",
        "print(\"Bi-LSTM modeli eğitiliyor...\")\n",
        "bilstm_model = Sequential([\n",
        "    Embedding(input_dim=len(bert_tokenizer.vocab), output_dim=128, input_length=128),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(one_hot_labels.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "bilstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "bilstm_history = bilstm_model.fit(X_train_embeddings, y_train, epochs=10, batch_size=16, validation_data=(X_test_embeddings, y_test))\n",
        "# Modeller ve Değerlendirme\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Tahminler ve performans metrikleri hesaplama\n",
        "def evaluate_model_performance(models, X_test, y_test):\n",
        "    y_true_classes = np.argmax(y_test, axis=1)  # Gerçek sınıflar\n",
        "    results = []\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        y_pred = model.predict(X_test)  # Model tahmini\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)  # Tahmini sınıflar\n",
        "\n",
        "        # Performans metrikleri hesaplama\n",
        "        precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "        accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "\n",
        "        # Sonuçları kaydetme\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1-Score\": f1,\n",
        "            \"Accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Modeller ve değerlendirme\n",
        "models = {\n",
        "    \"CNN\": cnn_model,\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"GRU\": gru_model,\n",
        "    \"Bi-LSTM\": bilstm_model  # Bi-LSTM modelini de dahil ediyoruz\n",
        "}\n",
        "\n",
        "# Performans sonuçlarını hesapla ve göster\n",
        "metrics_df = evaluate_model_performance(models, X_test_embeddings, y_test)\n",
        "print(metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFD9-p2gV_gk",
        "outputId": "e337998f-c5f0-4f23-e5ec-d5cc58f847c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kullanılan cihaz: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Train Loss: 0.7546, Train Accuracy: 0.6657\n",
            "Validation Loss: 0.6383, Validation Accuracy: 0.7289\n",
            "Validation F1-Score: 0.7214\n",
            "Validation Precision: 0.7186\n",
            "Validation Recall: 0.7289\n",
            "Epoch 2/5\n",
            "Train Loss: 0.5951, Train Accuracy: 0.7497\n",
            "Validation Loss: 0.6495, Validation Accuracy: 0.7367\n",
            "Validation F1-Score: 0.7156\n",
            "Validation Precision: 0.7269\n",
            "Validation Recall: 0.7367\n",
            "Epoch 3/5\n",
            "Train Loss: 0.4501, Train Accuracy: 0.8125\n",
            "Validation Loss: 0.6640, Validation Accuracy: 0.7292\n",
            "Validation F1-Score: 0.7266\n",
            "Validation Precision: 0.7247\n",
            "Validation Recall: 0.7292\n",
            "Epoch 4/5\n",
            "Train Loss: 0.3051, Train Accuracy: 0.8834\n",
            "Validation Loss: 0.8009, Validation Accuracy: 0.7111\n",
            "Validation F1-Score: 0.7110\n",
            "Validation Precision: 0.7111\n",
            "Validation Recall: 0.7111\n",
            "Epoch 5/5\n",
            "Train Loss: 0.1842, Train Accuracy: 0.9309\n",
            "Validation Loss: 1.0902, Validation Accuracy: 0.7100\n",
            "Validation F1-Score: 0.7060\n",
            "Validation Precision: 0.7046\n",
            "Validation Recall: 0.7100\n",
            "Model test verisi üzerinde değerlendiriliyor...\n",
            "Test Loss: 1.0902, Test Accuracy: 0.7100\n",
            "Test F1-Score: 0.7060\n",
            "Test Precision: 0.7046\n",
            "Test Recall: 0.7100\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Cihazı Belirle\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Kullanılan cihaz: {device}\")\n",
        "\n",
        "# Veriyi Yükle\n",
        "df_cleaned = pd.read_csv('/content/processed_text.csv', encoding='utf-8')\n",
        "df_original = pd.read_csv('train_0.csv', encoding='utf-8')\n",
        "df_cleaned['Label'] = df_original['Label']\n",
        "\n",
        "# One-Hot Encoding\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_labels = one_hot_encoder.fit_transform(df_cleaned['Label'].values.reshape(-1, 1))\n",
        "\n",
        "# Eğitim ve Test Verisini Ayır\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_cleaned['Processed_Text'],\n",
        "    one_hot_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# BERT Tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
        "\n",
        "# Tokenizasyon ve Padleme\n",
        "def tokenize_and_pad(texts, tokenizer, max_len=128):\n",
        "    tokens = tokenizer(\n",
        "        texts.tolist(),\n",
        "        max_length=max_len,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "X_train_ids, X_train_mask = tokenize_and_pad(X_train, bert_tokenizer)\n",
        "X_test_ids, X_test_mask = tokenize_and_pad(X_test, bert_tokenizer)\n",
        "\n",
        "# Tensor Tipi Dönüşümü\n",
        "y_train = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)\n",
        "\n",
        "# TensorDataset Oluştur\n",
        "train_dataset = TensorDataset(X_train_ids, X_train_mask, y_train)\n",
        "test_dataset = TensorDataset(X_test_ids, X_test_mask, y_test)\n",
        "\n",
        "# DataLoader Oluştur\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# BERT Modeli ve Optimizasyon\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dbmdz/bert-base-turkish-cased\",\n",
        "    num_labels=len(one_hot_encoder.categories_[0])\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Eğitim Döngüsü\n",
        "def train_model(model, dataloader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for batch in dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n",
        "\n",
        "# Değerlendirme Döngüsü\n",
        "def evaluate_model(model, dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "    return total_loss / len(dataloader), total_correct / len(dataloader.dataset), report\n",
        "\n",
        "# Eğitim ve Değerlendirme Süreci\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_model(bert_model, train_loader, optimizer, loss_fn, device)\n",
        "    val_loss, val_accuracy, val_report = evaluate_model(bert_model, test_loader, loss_fn, device)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"Validation F1-Score: {val_report['weighted avg']['f1-score']:.4f}\")\n",
        "    print(f\"Validation Precision: {val_report['weighted avg']['precision']:.4f}\")\n",
        "    print(f\"Validation Recall: {val_report['weighted avg']['recall']:.4f}\")\n",
        "\n",
        "# Test Değerlendirmesi\n",
        "print(\"Model test verisi üzerinde değerlendiriliyor...\")\n",
        "test_loss, test_accuracy, test_report = evaluate_model(bert_model, test_loader, loss_fn, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1-Score: {test_report['weighted avg']['f1-score']:.4f}\")\n",
        "print(f\"Test Precision: {test_report['weighted avg']['precision']:.4f}\")\n",
        "print(f\"Test Recall: {test_report['weighted avg']['recall']:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIUgE3YVWD12"
      },
      "outputs": [],
      "source": [
        "# Tek Cümle Tahmini Fonksiyonu\n",
        "def predict_sentence(sentence, tokenizer, model, device):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(sentence, max_length=128, padding='max_length', truncation=True, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "        logits = outputs.logits if hasattr(outputs, 'logits') else model(inputs['input_ids'])\n",
        "    prediction = torch.argmax(logits, dim=1).cpu().item()\n",
        "    label = one_hot_encoder.categories_[0][prediction]\n",
        "    return label\n",
        "\n",
        "# Kullanıcıdan Cümle Al ve Tahmin Yap\n",
        "user_sentence = input(\"Bir cümle yazın: \")\n",
        "predicted_label = predict_sentence(user_sentence, bert_tokenizer, bert_model, device)\n",
        "print(f\"Tahmin Edilen Etiket: {predicted_label}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}